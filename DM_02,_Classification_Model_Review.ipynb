{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju75-K0IIIUh"
   },
   "source": [
    "We begin by importing the same libraries as in the first notebook of the sequence. Remember to make sure that you have these all installed in your environment by the end of your first lab. It is perfectly fine if someone walks you through this, but it is essential that this is done to complete the rest of the work for this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUxIkvKyHpYW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsrf9JtZIjVw"
   },
   "source": [
    "Next we load the iris dataset, just as in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doIZXZk0IIDT"
   },
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "target_data = np.reshape(iris_data.target, (150, 1))\n",
    "all_iris_data = np.concatenate((iris_data.data, target_data), axis = 1)\n",
    "\n",
    "iris_df = pd.DataFrame(all_iris_data,\n",
    "                       columns = ['sepal length', 'sepal width',\n",
    "                                  'petal length', 'petal width', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "7HTKm2DXIczf",
    "outputId": "ba398773-2f8d-4a2d-ded1-2c9737a0157e"
   },
   "outputs": [],
   "source": [
    "# take a look at the data we imported\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7pfUfAhJihq"
   },
   "source": [
    "In the future, when you deal with larger datasets, you can save them to a csv or pickle file and access that later. You will need to specify the path your file is saved to and later accessed from. This path will vary for each of you. If this is new to you, and you have some free time during the lab, you could ask someone to show you how this works.\n",
    "\n",
    "Now that we have the data, let's get to our main task for today: we will train a classification model to predict the target value. Remember that this is the fifth value in each row, classifying the flower in one of three possible classes: setosa (denoted by 0), versicolor (denoted by 1), or virginica (denoted by 2). For example, inspect the data in the first row of the dataset and make sure you know what each value corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZaXE0EgJXAa"
   },
   "outputs": [],
   "source": [
    "# Q1: print the data in row 0. What do the numbers you see represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReXJ7DNYRQPF"
   },
   "source": [
    "We can also print some more information on the range of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "WJ__LhbtLuiJ",
    "outputId": "06c8673e-a3b0-44c5-8ed1-6998166ba081"
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4ThLEuuSXjI"
   },
   "source": [
    "There are several possible approaches to create a classification model. If you have done something like this before, you could give this a try yourself and then come back if you'd like to see a possible solution. Multiple different solutions can be correct.\n",
    "\n",
    "It is useful to start by taking a closer look at the data to get some insight. We will begin by generating some plots using a library called seaborn. Again, this will need to be installed before you can import it. Once you have generated your plots, spend some time looking at them. Make sure you can explain what each plot represents. As always, ask a TA or classmate if you are unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1uOMursTlv-"
   },
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEQCmY3CT36_"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dIZo-ylET39N",
    "outputId": "76d05b5e-603e-4a1a-d30c-26a3f5304b58"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris_df, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmr8XisBYUVW"
   },
   "source": [
    "Before we begin training, we create datasets called X consisting of input variables and y consisting of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_T3sIUHYh7o"
   },
   "outputs": [],
   "source": [
    "X = iris_df.drop(['target'], axis=1)\n",
    "y = iris_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b1ne23gYtwM"
   },
   "outputs": [],
   "source": [
    "# Q3: feel free to print these to check that what we have created is what we expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6OhWYlZSXlm"
   },
   "source": [
    "In order to train a model, one usually splits the data into two groups: we use part of the data for the actual training process (\"training set\") and part of it to test our model afterwards (\"testing set\"). Sometimes a third group is considered, referred to as validation set. We will not make use of it now, but if you have time and haven't encountered this before you could look this up as it can be very useful for more complicated models.\n",
    "\n",
    "To split our data into training and testing sets, we use a library called train_test_split. Let's import it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-qJG6FsT3_W"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7hCwT5vSXnp"
   },
   "source": [
    "Train_test_split automatically shuffles the data before it is divided into two groups by setting shuffle = True.\n",
    "\n",
    "Q2: Can you see why this is important for us?\n",
    "\n",
    "We will use train_test_split with test_size = 0.3 to split the data into a training set consiting of 70% of the data and a testing set consisting of 30% of the data. I will also set random_state = 123. This just ensures that if we want to replicate the split, the same two sets are created. Since the split is done randomly, without setting this parameter you'll have different results every time you repeat this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9oRm_CzROpE"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFXe9WqAROq-"
   },
   "outputs": [],
   "source": [
    "# Q4: once again, if you haven't done this a lot before, print the training and testing sets we\n",
    "# just created to make sure they look the way you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFB1CxI1b0L9"
   },
   "source": [
    "We will use a SVC (Support Vector Classifier) algorithm to create our model. For now, you may treat this as a black box, but if you are curious to learn a little bit more about why this works, you can read more about this online. If you are already familiar with these ideas, you can also explore other supervised learning algorithms, and maybe try a few and see which gives you the best results. Many supervised learning algorithms, including SVC, can be implemented easily in sklearn. Let's start by importing the library that allows us to train a SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtX6ROXPawYU"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkgAIpvCc2gP"
   },
   "source": [
    "We can now create a model by specifying a few parameters. For now, you can use the parameters I chose, or you can head here https://scikit-learn.org/stable/modules/svm.html to read more about what each of these does and try out a few other combinations. You may even find one that gives better results than this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6y_zxDefawaz"
   },
   "outputs": [],
   "source": [
    "model = SVC(C=1, kernel='rbf', tol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPEIHU2pdW3T"
   },
   "source": [
    "We're ready to train our model using our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "p4VY5VWkawcW",
    "outputId": "4141ee9f-653c-46f5-db0a-c91d4fce308d"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4AdPTpsde-k"
   },
   "source": [
    "It is now very important that we look back and evaluate our model, to see how well it is performing. Luckily, sklearn gives us several built in tools that we can use to do this. Let's import a few. You'll find more information on each of them here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmbmezV8ROtT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w51tAmBIeJKr"
   },
   "source": [
    "Let's apply the model to our testing set, creating a vector of predicted values. These can then be compared with the true values in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b0Oj8Gtd0LU"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-aetl31ed2w"
   },
   "source": [
    "Let's start computing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US4GIkmJd0Nw",
    "outputId": "fc915b48-6666-43b2-870b-6348a29e7970"
   },
   "outputs": [],
   "source": [
    "print('Accuracy score: ', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFeN_NjRet65"
   },
   "source": [
    "Now let's look at the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gXOKyj8etPo",
    "outputId": "fb7efc76-98d0-4497-8a8a-d72c981d5dbc"
   },
   "outputs": [],
   "source": [
    "print('MSE: ', mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibwdjHvCfz8Z"
   },
   "source": [
    "Q5: Take a second to look at your accuracy score and mean squared error. What do these numbers represent?\n",
    "\n",
    "Hopefully this gave you an overview of what's involved in preparing the data, training and evaluating a model. We encourage you to use this as a starting point for discussion with your TAs and classmates and for further readings on the parts of this process that you are less familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWRx2xASLiz_"
   },
   "outputs": [],
   "source": [
    "# ANS1:\n",
    "# iris_df.loc[[0]]\n",
    "## sepal length, sepal width, petal length and petal width have numerical values,\n",
    "## the target value 0.0 means that the flower in row 0 is a setosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUAEZfJgWw6A"
   },
   "source": [
    "ANS 2: Take a look at the dataframe iris_df. All data is ordered by the target value: we have 50 setosa first, then 50 versicolor, then 50 virginica. If the data wasn't shuffled before splitting it, we'd have an uneven representation of the three classes in the training and testing sets. For example, let's say we take the first two thirds of data for training and the remaining third for testing. We'll end up with a trainign set made up of all setosa and all versicolor data points, and a testing set made up of only virginica data points. This can't be good to train a model that's supposed to be able to recognise all three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UHQBHGlWzKd"
   },
   "outputs": [],
   "source": [
    "# ANS3:\n",
    "# print(X)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRyjieznY6CU"
   },
   "outputs": [],
   "source": [
    "# ANS4:\n",
    "# print(\"X_train:\")\n",
    "# print(X_train)\n",
    "# print(\"X_test:\")\n",
    "# print(X_test)\n",
    "# print(\"y_train:\")\n",
    "# print(y_train)\n",
    "# print(\"y_test:\")\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDQvvd5b4tJ2"
   },
   "source": [
    "ANS 5: The exact values depend on some choices made during training, so they will vary from run to run. You probably got a value or roughly 0.9 or a little higher for accuracy. An accuracy of 0.9 means that our model correctly classifies 90% of the data. You also probably got a value slightly less than 0.1 for MSE, which is consistent with about 90% of our predictions being correct."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
